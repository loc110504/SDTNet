# SDT-Net: Dynamic Teacher Switching with Hierarchical Consistency for Scribble-Supervised Medical Image Segmentation
Official code implementation of "SDT-Net: Dynamic Teacher Switching with Hierarchical Consistency for Scribble-Supervised Medical Image Segmentation"
## Introduction
Scribble-supervised methods have emerged to mitigate the prohibitive annotation burden in medical image segmentation. However, the inherent sparsity of these annotations introduces significant ambiguity, which results in noisy pseudo-label propagation and hinders the learning of robust anatomical boundaries. To address this challenge, we propose SDTNet, a novel dual-teacher, single-student framework designed to maximize supervision quality from these weak signals. Our method features a Dynamic Teacher Switching (DTS) module to adaptively select the most reliable teacher. This selected teacher then guides the student via two synergistic mechanisms: high-confidence pseudo-labels, refined by a Pick Reliable Pixels (PRP) mechanism, and multi-level feature alignment, enforced by a Hierarchical Consistency (HiCo) module. Extensive experiments on the ACDC and MSCMRseg datasets demonstrate that SDTNet achieves state-of-the-art performance, producing more accurate and anatomically plausible segmentations.
![Training Process](framework.png)

## Datasets
### â¤ï¸ ACDC Dataset
- Mask Annotations: [ACDC](https://www.creatis.insa-lyon.fr/Challenge/acdc/) 
- Scribble annotations: [ACDC scribbles](https://vios-s.github.io/multiscale-adversarial-attention-gates/data)
### ğŸ«€ MSCMR Dataset
- Mask Annotations: [MSCMRseg](https://zmiclab.github.io/zxh/0/mscmrseg19/data.html)
- Scribble annotations: [MSCMR_scribbles](https://github.com/BWGZK/CycleMix/tree/main/MSCMR_scribbles)
- Scribble-annotated dataset for training: [MSCMR_dataset](https://github.com/BWGZK/CycleMix/tree/main/MSCMR_dataset). 

> We have reorganized the datasets, and they are now available for download at: ğŸ‘‰ [Google Drive](https://drive.google.com/drive/folders/1OCPCEKdMr7Gh9v7xhSY5c_HF1e0TRkDL?usp=sharing)

## Setup
1. Clone this project and create a conda environment
```bash
git clone https://github.com/loc110504/SDTNET.git
cd SDTNET

conda create -n sdtnet python=3.10.18
conda activate sdtnet
```
2. Install requirements and packages
```bash
pip install -r requirements.txt
```
## Usage
### For training:

```bash
cd code/train
bash run.sh
```

### For testing:
```bash
cd code/test
python test_acdc.py # for ACDC dataset
python test_mscmr.py # for MSCMRseg dataset
```

### Configuration
The `run.sh` file contains several configurable parameters for training experiments, such as:
- Dataset path and type (e.g., ACDC, MSCMR)
- Training hyperparameters like learning rate, batch size, and total iterations
- Experimental settings such as seed, GPU index, and supervision type (e.g., scribble)
You can modify these arguments in `run.sh` to adjust the training setup for your experiments.

## Project Structure
```
â”œâ”€â”€ ğŸ“ code
â”‚   â”œâ”€â”€ ğŸ“ dataloader
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ acdc.py
â”‚   â”‚   â””â”€â”€ mscmr.py
â”‚   â”œâ”€â”€ ğŸ“ networks
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ net_factory.py
â”‚   â”‚   â””â”€â”€ unet.py
â”‚   â”œâ”€â”€ ğŸ“ test
â”‚   â”‚   â”œâ”€â”€ test_mscmr.py
â”‚   â”‚   â”œâ”€â”€ test_acdc.py
â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â”œâ”€â”€ ğŸ“ train
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ run.sh
â”‚   â”‚   â”œâ”€â”€ train_method_acdc.py
â”‚   â”‚   â””â”€â”€ train_method_mscmr.py
â”‚   â”œâ”€â”€ ğŸ“ utils
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ema_optim.py
â”‚   â”‚   â”œâ”€â”€ losses.py
â”‚   â”‚   â”œâ”€â”€ pick_reliable_pixels.py
â”‚   â”‚   â””â”€â”€ ramps.py
â”‚   â””â”€â”€ val.py
|
â”œâ”€â”€ ğŸ“ data
â”‚   â”œâ”€â”€ ğŸ“ ACDC
â”‚   â”œâ”€â”€ ğŸ“ MSCMR
|
â”œâ”€â”€ ğŸ“ README.md
â””â”€â”€ ğŸ“„ requirements.txt
```

## Acknowledgement
We gratefully acknowledge the public release of [WSL4MIS](https://github.com/HiLab-git/WSL4MIS) and [CycleMix](https://github.com/BWGZK/CycleMix) for part of their codes, processed datasets and data splits.
